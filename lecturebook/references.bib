@article{sahu2020,
   abstract = {The numerical calculation of the electromagnetic fields within a permanent magnet synchronous machine (PMSM) is sensitive to the magnetic characteristic of steel and permanent magnets. The objective of this work is to quantify the impact of magnetization curve uncertainties on the performance of a PMSM and its drive system. Our approach is as follows. First, we construct stochastic models that reflect the uncertainties in B-H curves and PM remanent flux density. Second, to overcome the computational limitations of finite element analysis, we replace it with surrogate models based on Gaussian process regression. Third, we perform uncertainty propagation studies to assess the combined effect of steel and PM uncertainties on the maximum average torque vs. speed characteristics and the minimum dc-link voltage.},
   author = {A. Sahu and D. Aliprantis and I. Bilionis},
   doi = {10.1109/TEC.2020.2998142},
   issn = {15580059},
   issue = {4},
   journal = {IEEE Transactions on Energy Conversion},
   keywords = {Ferromagnetic materials,Gaussian processes,motor drives,permanent magnet machines,permanent magnets,uncertainty},
   title = {Quantification and Propagation of Uncertainty in the Magnetic Characteristic of Steel and Permanent Magnets of a Synchronous Machine Drive},
   volume = {35},
   year = {2020},
}

@article{beltran2020,
   abstract = {The objective of this article is to set forth a computationally efficient methodology to quantify the effects and assess the relevance of geometric and material uncertainty on a permanent magnet synchronous machine (PMSM), based on a nonlinear finite-element model (FEM). Our methodology follows the theory of Gaussian process regression and principal component analysis to build a computationally inexpensive surrogate model that replaces the FEM. Uncertainty quantification and sensitivity analysis studies focus on the electromagnetic torque, flux linkage, and core loss of the PMSM.},
   author = {A. Beltrán-Pulido and D. Aliprantis and I. Bilionis and A.R. Munoz and F. Leonardi and S.M. Avery},
   doi = {10.1109/TEC.2020.3001914},
   issn = {15580059},
   issue = {4},
   journal = {IEEE Transactions on Energy Conversion},
   keywords = {Finite-element analysis,Gaussian processes,permanent magnet machines,principal component analysis,sensitivity analysis,uncertain systems},
   title = {Uncertainty Quantification and Sensitivity Analysis in a Nonlinear Finite-Element Model of a Permanent Magnet Synchronous Machine},
   volume = {35},
   year = {2020},
}


@article{AAKASH2019104085,
title = {Stress-strain data for aluminum 6061-T651 from 9 lots at 6 temperatures under uniaxial and plane strain tension},
journal = {Data in Brief},
volume = {25},
pages = {104085},
year = {2019},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.104085},
url = {https://www.sciencedirect.com/science/article/pii/S2352340919304391},
author = {B.S. Aakash and JohnPatrick Connors and Michael D. Shields},
keywords = {Aluminum, Variability, Thermo-mechanical material behavior, Temperature-dependence, Stress-strain},
abstract = {Stress-strain curves in steady-state tension of aluminum 6061-T651 sourced from 9 lots of material from several manufacturers at 6 temperatures (20,100,150,200,250,300 °C) are presented. A total of 100 stress-strain curves for uniaxial tension specimens and 54 stress-strain curves for plane strain tension specimens are shared. Strains are estimated through digital image correlation using a 50.8 mm (2 inch) gauge length for the uniaxial tension specimens and a 6.35 mm (0.25 inch) gauge length for the plane strain specimens. The strains are an average of several measurements (at approximately 350-μm intervals) across the width of the gauge section.}
}

@article{KATSOUNAROS2012270,
title = {Reaction pathways in the electrochemical reduction of nitrate on tin},
journal = {Electrochimica Acta},
volume = {71},
pages = {270-276},
year = {2012},
issn = {0013-4686},
doi = {https://doi.org/10.1016/j.electacta.2012.03.154},
url = {https://www.sciencedirect.com/science/article/pii/S0013468612005208},
author = {Ioannis Katsounaros and Maria Dortsiou and Christos Polatides and Simon Preston and Theodore Kypraios and Georgios Kyriacou},
keywords = {Nitrate, Reduction, Tin, Reaction pathways, Parameter inference},
abstract = {The reaction pathways that lead to the formation of intermediates and final products of the reduction of nitrate on tin at high overpotential were studied in this paper. Possible chemical or electrochemical reactions of the intermediates were investigated and discussed. A complex mechanistic scheme was proposed which describes the formation of all products apart from nitrogen, even though the latter is the main electrolysis product. Several simplified reaction schemes were investigated and each fitted to experimental data using a Bayesian approach. It was concluded that in order to describe the rate of nitrogen formation, another intermediate must be considered between nitrite and nitrogen. It is finally postulated that this intermediate is nitramide; however, further work in order to develop a method for determining the nitramide concentration is required to confirm that this is indeed the precursor of nitrogen.}
}

@book{jaynes03,
  added-at = {2011-05-09T23:10:52.000+0200},
  address = {Cambridge},
  author = {Jaynes, E. T.},
  biburl = {https://www.bibsonomy.org/bibtex/2ed3616cca9af65830fb13b9f53e0f19b/josephausterwei},
  interhash = {27c58f26b65cfde811cbc41b7fe319cd},
  intrahash = {ed3616cca9af65830fb13b9f53e0f19b},
  keywords = {imported},
  publisher = {Cambridge University Press},
  timestamp = {2011-05-10T10:42:42.000+0200},
  title = {Probability theory: The logic of science},
  year = 2003
}

@article{VANHORN20033,
title = {Constructing a logic of plausible inference: a guide to Cox’s theorem},
journal = {International Journal of Approximate Reasoning},
volume = {34},
number = {1},
pages = {3-24},
year = {2003},
issn = {0888-613X},
doi = {https://doi.org/10.1016/S0888-613X(03)00051-3},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X03000513},
author = {Kevin S {Van Horn}},
keywords = {Cox, Bayesian, Probability},
abstract = {Cox’s theorem provides a theoretical basis for using probability theory as a general logic of plausible inference. The theorem states that any system for plausible reasoning that satisfies certain qualitative requirements intended to ensure consistency with classical deductive logic and correspondence with commonsense reasoning is isomorphic to probability theory. However, the requirements used to obtain this result have been the subject of much debate. We review Cox’s theorem, discussing its requirements, the intuition and reasoning behind these, and the most important objections, and finish with an abbreviated proof of the theorem.}
}

@book{robert_monte_2004,
  added-at = {2020-07-15T00:37:54.000+0200},
  author = {Robert, C.P. and Casella, G.},
  biburl = {https://www.bibsonomy.org/bibtex/2c45ba78e01455a35b301daddc182caed/twagener},
  interhash = {faaab58e84fb4966b7d2118bc839c076},
  intrahash = {c45ba78e01455a35b301daddc182caed},
  keywords = {},
  publisher = {Springer Verlag},
  timestamp = {2020-07-15T00:37:54.000+0200},
  title = {Monte {Carlo} statistical methods},
  year = 2004
}

@book{bishop2006, author = {Bishop, Christopher M.}, title = {Pattern Recognition and Machine Learning (Information Science and Statistics)}, year = {2006}, isbn = {0387310738}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg} }

@article{shannon,
  added-at = {2021-09-19T18:40:37.000+0200},
  author = {Shannon, Claude Elwood},
  biburl = {https://www.bibsonomy.org/bibtex/29f88587b33c82f692b61d129eb2f2517/steschum},
  interhash = {754130207906fcec16a53d330eeff348},
  intrahash = {9f88587b33c82f692b61d129eb2f2517},
  journal = {The Bell System Technical Journal},
  keywords = {imported},
  pages = {379--423},
  timestamp = {2021-09-19T18:41:56.000+0200},
  title = {A Mathematical Theory of Communication},
  url = {http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
  urldate = {2003-04-22},
  volume = 27,
  year = 1948
}

@book{10.7551/mitpress/3206.001.0001,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    title = "{Gaussian Processes for Machine Learning}",
    publisher = {The MIT Press},
    year = {2005},
    month = {11},
    abstract = "{A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.}",
    isbn = {9780262256834},
    doi = {10.7551/mitpress/3206.001.0001},
    url = {https://doi.org/10.7551/mitpress/3206.001.0001},
}

@inproceedings{Diaconis1988BayesianNA,
  title={Bayesian numerical analysis},
  author={Persi Diaconis},
  year={1988},
  url={https://api.semanticscholar.org/CorpusID:123740858}
}

@article{3230516c-5e5c-34ef-aef1-442899abfb0a,
 ISSN = {00401706, 15372723},
 URL = {http://www.jstor.org/stable/40586652},
 abstract = {Mathematical models, usually implemented in computer programs known as simulators, are widely used in all areas of science and technology to represent complex real-world phenomena. Simulators are often so complex that they take appreciable amounts of computer time or other resources to run. In this context, a methodology has been developed based on building a statistical representation of the simulator, known as an emulator. The principal approach to building emulators uses Gaussian processes. This work presents some diagnostics to validate and assess the adequacy of a Gaussian process emulator as surrogate for the simulator. These diagnostics are based on comparisons between simulator outputs and Gaussian process emulator outputs for some test data, known as validation data, defined by a sample of simulator runs not used to build the emulator. Our diagnostics take care to account for correlation between the validation data. To illustrate a validation procedure, we apply these diagnostics to two different data sets.},
 author = {Leonardo S. Bastos and Anthony O'Hagan},
 journal = {Technometrics},
 number = {4},
 pages = {425--438},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Diagnostics for Gaussian Process Emulators},
 urldate = {2023-10-15},
 volume = {51},
 year = {2009}
}

@article{Frazier2018ATO,
  title={A Tutorial on Bayesian Optimization},
  author={P. Frazier},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.02811},
  url={https://api.semanticscholar.org/CorpusID:49656213}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{robbins-monro-1951,
author = {Herbert Robbins and Sutton Monro},
title = {{A Stochastic Approximation Method}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {400 -- 407},
year = {1951},
doi = {10.1214/aoms/1177729586},
URL = {https://doi.org/10.1214/aoms/1177729586}
}

@ARTICLE{lagaris1998,
  author={Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Artificial neural networks for solving ordinary and partial differential equations}, 
  year={1998},
  volume={9},
  number={5},
  pages={987-1000},
  doi={10.1109/72.712178}}

@article{raissi2019,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{karumuri2020,
title = {Simulator-free solution of high-dimensional stochastic elliptic partial differential equations using deep neural networks},
journal = {Journal of Computational Physics},
volume = {404},
pages = {109120},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.109120},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119308253},
author = {Sharmila Karumuri and Rohit Tripathy and Ilias Bilionis and Jitesh Panchal},
keywords = {Elliptic stochastic partial differential equations, Deep residual network, Physics-informed loss function, Energy functional, High-dimensional uncertainty propagation, Inverse problems},
abstract = {Stochastic partial differential equations (SPDEs) are ubiquitous in engineering and computational sciences. The stochasticity arises as a consequence of uncertainty in input parameters, constitutive relations, initial/boundary conditions, etc. Because of these functional uncertainties, the stochastic parameter space is often high-dimensional, requiring hundreds, or even thousands, of parameters to describe it. This poses an insurmountable challenge to response surface modeling since the number of forward model evaluations needed to construct an accurate surrogate grows exponentially with the dimension of the uncertain parameter space; a phenomenon referred to as the curse of dimensionality. State-of-the-art methods for high-dimensional uncertainty propagation seek to alleviate the curse of dimensionality by performing dimensionality reduction in the uncertain parameter space. However, one still needs to perform forward model evaluations that potentially carry a very high computational burden. We propose a novel methodology for high-dimensional uncertainty propagation of elliptic SPDEs which lifts the requirement for a deterministic forward solver. Our approach is as follows. We parameterize the solution of the elliptic SPDE using a deep residual network (ResNet). In a departure from traditional squared residual (SR) based loss function for training the ResNet, we introduce a physics-informed loss function derived from variational principles. Specifically, our loss function is the expectation of the energy functional of the PDE over the stochastic variables. We demonstrate our solver-free approach through various examples where the elliptic SPDE is subjected to different types of high-dimensional input uncertainties. Also, we solve high-dimensional uncertainty propagation and inverse problems.}
}

@article{yang2021,
title = {B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data},
journal = {Journal of Computational Physics},
volume = {425},
pages = {109913},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2020.109913},
url = {https://www.sciencedirect.com/science/article/pii/S0021999120306872},
author = {Liu Yang and Xuhui Meng and George Em Karniadakis},
keywords = {Nonlinear PDEs, Noisy data, Bayesian physics-informed neural networks, Hamiltonian Monte Carlo, Variational inference},
abstract = {We propose a Bayesian physics-informed neural network (B-PINN) to solve both forward and inverse nonlinear problems described by partial differential equations (PDEs) and noisy data. In this Bayesian framework, the Bayesian neural network (BNN) combined with a PINN for PDEs serves as the prior while the Hamiltonian Monte Carlo (HMC) or the variational inference (VI) could serve as an estimator of the posterior. B-PINNs make use of both physical laws and scattered noisy measurements to provide predictions and quantify the aleatoric uncertainty arising from the noisy data in the Bayesian framework. Compared with PINNs, in addition to uncertainty quantification, B-PINNs obtain more accurate predictions in scenarios with large noise due to their capability of avoiding overfitting. We conduct a systematic comparison between the two different approaches for the B-PINNs posterior estimation (i.e., HMC or VI), along with dropout used for quantifying uncertainty in deep neural networks. Our experiments show that HMC is more suitable than VI with mean field Gaussian approximation for the B-PINNs posterior estimation, while dropout employed in PINNs can hardly provide accurate predictions with reasonable uncertainty. Finally, we replace the BNN in the prior with a truncated Karhunen-Loève (KL) expansion combined with HMC or a deep normalizing flow (DNF) model as posterior estimators. The KL is as accurate as BNN and much faster but this framework cannot be easily extended to high-dimensional problems unlike the BNN based framework.}
}

@article{metropolis1953,
title = {Equation of state calculations by fast computing machines},
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
abstractNote = {},
doi = {10.2172/4390578},
url = {https://www.osti.gov/biblio/4390578}, journal = {},
place = {United States},
year = {1953},
month = {3}
}